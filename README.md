

# Informe Final ‚Äì Sistema de Interacci√≥n Visual sin Contacto


### Datos de la estudiante

-   **Nombre completo:** Mar√≠a Fernanda Cala Rodr√≠guez
    
-   **N√∫mero de documento:** 1027522940
    
-   **Correo institucional:** mcalar@unal.edu.co
      


## üß© Requisitos del Proyecto

### 1. üìå Definici√≥n del Problema

En entornos donde se llevan a cabo reuniones de equipo, clases o actividades colaborativas, es com√∫n depender de interfaces f√≠sicas (teclados, mouse, pantallas t√°ctiles) para interactuar con contenidos digitales. Sin embargo, existen escenarios donde el uso de estas interfaces no es pr√°ctico, ya sea por razones de accesibilidad, higiene o conveniencia.

Este sistema se concibi√≥ pensando en su aplicaci√≥n dentro de **metodolog√≠as √°giles**, donde la **rapidez**, la **adaptabilidad** y la **colaboraci√≥n** son fundamentales. Al eliminar la necesidad de contacto f√≠sico con los dispositivos, facilitamos interacciones m√°s fluidas y din√°micas en reuniones de equipo, sesiones de _brainstorming_ o presentaciones interactivas. La capacidad de controlar la interfaz con **gestos y comandos de voz** permite a los usuarios interactuar con el contenido de manera m√°s natural y eficiente, reduciendo fricciones y optimizando el tiempo, elementos clave para mantener la agilidad en cualquier proyecto.

Este proyecto busca dise√±ar un sistema que permita controlar funciones b√°sicas como cambiar de vista, dibujar, mover objetos o ejecutar acciones mediante **interacci√≥n visual sin contacto**, utilizando una c√°mara y comandos de voz.

El problema es relevante dentro del campo de la **computaci√≥n visual** porque permite aplicar t√©cnicas avanzadas de detecci√≥n de objetos, seguimiento de manos, an√°lisis de gestos y reconocimiento de voz en tiempo real. Adem√°s, promueve el desarrollo de interfaces naturales y accesibles que responden al entorno y al usuario sin necesidad de contacto f√≠sico.

----------

### 2. üß† Selecci√≥n de Talleres

Se integran cinco (5) talleres desarrollados durante el curso:

**2025-06-21_taller_sistema_monitoreo_inteligente_vision_dashboard**
Para detectar si hay una persona y activar/desactivar la interfaz

**2025-05-17_taller_gestos_webcam_mediapipe**
Para implementar el manejo de interfaz con gestos.

**2025-05-24_taller_reconocimiento_voz_local**
Para tener una manera de escribir sin contacto se utiliza la voz.

**2025-06-18_taller_pintura_interactiva_voz_gestos**
Para implementaci√≥n de formas para dibujar con gestos.

**2025-06-23_taller_monitor_visual_3d_integracion_python**
Para seguimiento de las manos.

**T√©cnicas y herramientas utilizadas:**

-   Modelo **YOLOv8** (versi√≥n ligera `yolov8n.pt`) para detecci√≥n de personas.
    
-   Biblioteca **`MediaPipe`** para reconocimiento de manos y gestos.
    
-   **`SpeechRecognition`** y **`pyttsx3`** para entrada y salida por voz.
    
-   **`OpenCV`** para la manipulaci√≥n de video, visualizaci√≥n y gr√°ficos.
    
-   Programaci√≥n modular en Python con estructuras separadas por funci√≥n.
    

----------

### 3. üèóÔ∏è Arquitectura de Soluci√≥n

![diagrama](informe/gifs/diagrama.png)

#### Descripci√≥n General

El sistema est√° compuesto por cuatro m√≥dulos principales que trabajan de forma concurrente:

1.  **Captura de video** desde la webcam, gestionada por el m√≥dulo principal (`main.py`).
    
2.  **Detecci√≥n de personas** con YOLOv8 para activar o desactivar la interfaz.
    
3.  **Reconocimiento de gestos** con MediaPipe para cambiar de modo, seleccionar objetos, dibujar.
    
4.  **Reconocimiento de voz** para escribir de forma alternativa sin necesidad de contacto.

----------

### Enlace al video

https://www.youtube.com/watch?v=NSueTlPPVJg

### Explicaci√≥n t√©cnica del funcionamiento

-   `main.py` administra la captura de video y coordina los m√≥dulos.
    
-   `detection.py` emplea YOLOv8 para identificar personas en la escena.
    
-   `gesture_control.py` gestiona el cambio de escenas y acciones visuales mediante gestos.
    
-   `voice_control.py` ejecuta reconocimiento de voz cada ciertos segundos y realiza retroalimentaci√≥n hablada.
    
### Evidencia de funcionamiento

Crear nueva tarea y dictado por voz:
![GIF](informe/gifs/nueva.gif)

Marcar tarea como completada y eliminar:
![GIF](informe/gifs/completada.gif)

Modo dibujo:
![GIF](informe/gifs/dibujo.gif)

Editar tarea:
![GIF](informe/gifs/editar.gif)

Inactividad/Reactivar al detectar persona:
![GIF](informe/gifs/inactiva.gif)


### Conclusiones y reflexiones personales

Este proyecto integr√≥ m√∫ltiples conceptos y herramientas vistas en clase, permitiendo desarrollar un sistema que responde a est√≠mulos visuales y auditivos sin necesidad de contacto f√≠sico.

Adem√°s de reforzar conocimientos en visi√≥n por computadora, redes neuronales y procesamiento de video, se aprendi√≥ a estructurar un sistema modular, reutilizable y eficiente en tiempo real.

La experiencia fue valiosa tanto desde el punto de vista t√©cnico como en t√©rminos de dise√±o de interfaces accesibles y pr√°cticas para entornos reales.
